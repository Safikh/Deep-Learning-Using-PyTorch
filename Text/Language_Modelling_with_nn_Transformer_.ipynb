{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language Modelling with nn.Transformer .ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cE6vdtLk5u2W"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder as TE, TransformerEncoderLayer as TEL \n",
        "from torch.utils.data import dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PositionalEncoding module injects some information about the relative or absolute position of the tokens in the sequence. The positional encodings have the same dimension as the embeddings so that the two can be summed. Here, we use sine and cosine functions of different frequencies."
      ],
      "metadata": {
        "id": "7VleU_TkMHLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model: int, dropout: float = 0.1, max_len:int = 5000):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    position = torch.arange(max_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "    pe = torch.zeros(max_len, 1, d_model)\n",
        "    pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  \n",
        "  def forward(self, x: Tensor) -> Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "    \"\"\"\n",
        "    x = x + self.pe[:x.size(0)]\n",
        "    return self.dropout(x)"
      ],
      "metadata": {
        "id": "mUUgDLQ3MBLW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "\n",
        "  def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
        "               nlayers: int, dropout: float = 0.5):\n",
        "    super().__init__()\n",
        "    self.model_type = 'Transformer'\n",
        "    self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "    encoder_layers = TEL(d_model, nhead, d_hid, dropout)\n",
        "    self.transformer_encoder = TE(encoder_layers, nlayers)\n",
        "    self.encoder = nn.Embedding(ntoken, d_model)\n",
        "    self.d_model = d_model\n",
        "    self.decoder = nn.Linear(d_model, ntoken)\n",
        "\n",
        "    self.init_weights()\n",
        "  \n",
        "\n",
        "  def init_weights(self) -> None:\n",
        "    initrange = 0.1\n",
        "    self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "    self.decoder.bias.data.zero_()\n",
        "    self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "  \n",
        "\n",
        "  def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      src: Tensor, shape [seq_len, batch_size]\n",
        "      src_mask: Tensor, shape [seq_len, seq_len]\n",
        "\n",
        "    Returns:\n",
        "      output Tensor of shape [seq_len, batch_size, ntoken]\n",
        "    \"\"\"\n",
        "\n",
        "    src = self.encoder(src) * math.sqrt(self.d_model)\n",
        "    src = self.pos_encoder(src)\n",
        "    output = self.transformer_encoder(src, src_mask)\n",
        "    output = self.decoder(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "a3nFR1Zp6Ydp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
        "  \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
        "  return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
      ],
      "metadata": {
        "id": "Wto-T92yL_nS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator"
      ],
      "metadata": {
        "id": "LZMQRKXEXXsw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter = WikiText2(split='train')\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEHkFPN4XkMs",
        "outputId": "59da2189-d99b-468e-b95f-698972b77bd2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.48M/4.48M [00:01<00:00, 2.90MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
        "  \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "  data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "  return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
      ],
      "metadata": {
        "id": "KK8CHUaQX_hH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_iter was \"consumed\" by the process of building the vocab, so we have to create it again\n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "train_data = data_process(train_iter)\n",
        "val_data = data_process(val_iter)\n",
        "test_data = data_process(test_iter)"
      ],
      "metadata": {
        "id": "VVTAD39JYfYP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "UdneZxk_YzMI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
        "  \"\"\"Divides the data into bsz separate sequences, removing extra elements that wouldn't cleanly fit.\n",
        "\n",
        "  Args:\n",
        "    data: Tensor, shape [N]\n",
        "    bsz: int, batch size\n",
        "\n",
        "  Returns:\n",
        "    Tensor of shape [N // bsz, bsz]\n",
        "  \"\"\"\n",
        "\n",
        "  seq_len = data.size(0) // bsz\n",
        "  data = data[:seq_len * bsz]\n",
        "  data = data.view(bsz, seq_len).t().contiguous()\n",
        "  return data.to(device)"
      ],
      "metadata": {
        "id": "2LxrULpTY1Ez"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_data, batch_size)\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ],
      "metadata": {
        "id": "AzlnvEuAdP2l"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bptt = 35\n",
        "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    source: Tensor, shape [full_seq_len, batch_size]\n",
        "    i: int\n",
        "\n",
        "  Returns:\n",
        "    tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "    target has shape [seq_len * batch_size]\n",
        "  \"\"\"\n",
        "\n",
        "  seq_len = min(bptt, len(source) - 1 - i)\n",
        "  data = source[i:i+seq_len]\n",
        "  target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "  return data, target\n"
      ],
      "metadata": {
        "id": "dnV71pbrduat"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ntokens = len(vocab)  # size of vocabulary\n",
        "emsize = 200  # embedding dimension\n",
        "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2  # number of heads in nn.MultiheadAttention\n",
        "dropout = 0.2  # dropout probability\n",
        "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
      ],
      "metadata": {
        "id": "rA1xCEZdfOPB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import time"
      ],
      "metadata": {
        "id": "QVNLb0KyfTlB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
      ],
      "metadata": {
        "id": "91BBIp6PfgXB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model: nn.Module) -> None:\n",
        "  \n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  log_interval = 200\n",
        "  start_time = time.time()\n",
        "  src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "\n",
        "  num_batches = len(train_data) // bptt\n",
        "  for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "    data, targets = get_batch(train_data, i)\n",
        "    batch_size = data.size(0)\n",
        "    if batch_size != bptt: # Only on last batch\n",
        "      src_mask = src_mask[:batch_size, :batch_size]\n",
        "    output = model(data, src_mask)\n",
        "    loss = criterion(output.view(-1, ntokens), targets)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    if batch % log_interval == 0 and batch > 0:\n",
        "      lr = scheduler.get_last_lr()[0]\n",
        "      ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "      cur_loss = total_loss / log_interval\n",
        "      ppl = math.exp(cur_loss)\n",
        "      print(f'| Epoch {epoch:3d} | batch [{batch:5d}/{num_batches:5d}] | '\n",
        "            f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "            f'loss: {cur_loss:5.2f} | ppl {ppl:8.2f}'\n",
        "      )\n",
        "      total_loss = 0\n",
        "      start_time = time.time()"
      ],
      "metadata": {
        "id": "RybXT1ZOfxgA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "  \n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "  src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "  \n",
        "    total_loss = 0\n",
        "  \n",
        "    for i in range(0, eval_data.size(0) - 1, bptt):\n",
        "      data, targets = get_batch(eval_data, i)\n",
        "      batch_size = data.size(0)\n",
        "      if batch_size != bptt:\n",
        "        src_mask = src_mask[:batch_size, :batch_size]\n",
        "      output = model(data, src_mask)\n",
        "      output_flat = output.view(-1, ntokens)\n",
        "      total_loss += batch_size * criterion(output_flat, targets).item()\n",
        "  \n",
        "  return total_loss / (len(eval_data) - 1)"
      ],
      "metadata": {
        "id": "ixC2HatriL-T"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "epochs = 10\n",
        "best_model = None\n",
        "for epoch in range(1, epochs + 1):\n",
        "  epoch_start_time = time.time()\n",
        "  train(model)\n",
        "  val_loss = evaluate(model, val_data)\n",
        "  val_ppl = math.exp(val_loss)\n",
        "  elapsed = time.time() - epoch_start_time\n",
        "  print('-' * 89)\n",
        "  print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "        f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "  print('-' * 89)\n",
        "\n",
        "  if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      best_model = copy.deepcopy(model)\n",
        "\n",
        "  scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gt9pziRBkZ5O",
        "outputId": "9398d684-6066-483e-a740-9e67515a9fb5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch   1 | batch [  200/ 2928] | lr 5.00 | ms/batch 38.97 | loss:  8.24 | ppl  3785.91\n",
            "| Epoch   1 | batch [  400/ 2928] | lr 5.00 | ms/batch 37.82 | loss:  6.91 | ppl  1000.46\n",
            "| Epoch   1 | batch [  600/ 2928] | lr 5.00 | ms/batch 37.88 | loss:  6.46 | ppl   640.40\n",
            "| Epoch   1 | batch [  800/ 2928] | lr 5.00 | ms/batch 37.83 | loss:  6.31 | ppl   547.71\n",
            "| Epoch   1 | batch [ 1000/ 2928] | lr 5.00 | ms/batch 37.71 | loss:  6.19 | ppl   488.78\n",
            "| Epoch   1 | batch [ 1200/ 2928] | lr 5.00 | ms/batch 37.74 | loss:  6.16 | ppl   474.24\n",
            "| Epoch   1 | batch [ 1400/ 2928] | lr 5.00 | ms/batch 38.14 | loss:  6.11 | ppl   449.03\n",
            "| Epoch   1 | batch [ 1600/ 2928] | lr 5.00 | ms/batch 37.81 | loss:  6.11 | ppl   448.64\n",
            "| Epoch   1 | batch [ 1800/ 2928] | lr 5.00 | ms/batch 37.65 | loss:  6.03 | ppl   413.77\n",
            "| Epoch   1 | batch [ 2000/ 2928] | lr 5.00 | ms/batch 37.76 | loss:  6.02 | ppl   413.40\n",
            "| Epoch   1 | batch [ 2200/ 2928] | lr 5.00 | ms/batch 37.76 | loss:  5.90 | ppl   363.39\n",
            "| Epoch   1 | batch [ 2400/ 2928] | lr 5.00 | ms/batch 37.72 | loss:  5.97 | ppl   392.51\n",
            "| Epoch   1 | batch [ 2600/ 2928] | lr 5.00 | ms/batch 37.68 | loss:  5.95 | ppl   385.60\n",
            "| Epoch   1 | batch [ 2800/ 2928] | lr 5.00 | ms/batch 37.54 | loss:  5.88 | ppl   357.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 114.57s | valid loss  5.79 | valid ppl   325.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| Epoch   2 | batch [  200/ 2928] | lr 4.75 | ms/batch 37.79 | loss:  5.87 | ppl   352.63\n",
            "| Epoch   2 | batch [  400/ 2928] | lr 4.75 | ms/batch 37.60 | loss:  5.85 | ppl   346.97\n",
            "| Epoch   2 | batch [  600/ 2928] | lr 4.75 | ms/batch 37.59 | loss:  5.67 | ppl   289.60\n",
            "| Epoch   2 | batch [  800/ 2928] | lr 4.75 | ms/batch 37.57 | loss:  5.70 | ppl   299.67\n",
            "| Epoch   2 | batch [ 1000/ 2928] | lr 4.75 | ms/batch 37.55 | loss:  5.65 | ppl   283.84\n",
            "| Epoch   2 | batch [ 1200/ 2928] | lr 4.75 | ms/batch 37.60 | loss:  5.68 | ppl   291.58\n",
            "| Epoch   2 | batch [ 1400/ 2928] | lr 4.75 | ms/batch 37.54 | loss:  5.69 | ppl   294.67\n",
            "| Epoch   2 | batch [ 1600/ 2928] | lr 4.75 | ms/batch 37.60 | loss:  5.71 | ppl   300.52\n",
            "| Epoch   2 | batch [ 1800/ 2928] | lr 4.75 | ms/batch 37.60 | loss:  5.65 | ppl   283.94\n",
            "| Epoch   2 | batch [ 2000/ 2928] | lr 4.75 | ms/batch 37.63 | loss:  5.67 | ppl   288.97\n",
            "| Epoch   2 | batch [ 2200/ 2928] | lr 4.75 | ms/batch 37.55 | loss:  5.55 | ppl   258.38\n",
            "| Epoch   2 | batch [ 2400/ 2928] | lr 4.75 | ms/batch 37.64 | loss:  5.64 | ppl   282.33\n",
            "| Epoch   2 | batch [ 2600/ 2928] | lr 4.75 | ms/batch 37.68 | loss:  5.64 | ppl   281.89\n",
            "| Epoch   2 | batch [ 2800/ 2928] | lr 4.75 | ms/batch 37.65 | loss:  5.58 | ppl   264.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 113.90s | valid loss  5.65 | valid ppl   285.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| Epoch   3 | batch [  200/ 2928] | lr 4.51 | ms/batch 37.77 | loss:  5.60 | ppl   271.00\n",
            "| Epoch   3 | batch [  400/ 2928] | lr 4.51 | ms/batch 37.58 | loss:  5.62 | ppl   275.40\n",
            "| Epoch   3 | batch [  600/ 2928] | lr 4.51 | ms/batch 37.57 | loss:  5.42 | ppl   225.36\n",
            "| Epoch   3 | batch [  800/ 2928] | lr 4.51 | ms/batch 37.52 | loss:  5.48 | ppl   240.80\n",
            "| Epoch   3 | batch [ 1000/ 2928] | lr 4.51 | ms/batch 37.55 | loss:  5.44 | ppl   231.59\n",
            "| Epoch   3 | batch [ 1200/ 2928] | lr 4.51 | ms/batch 37.56 | loss:  5.49 | ppl   241.09\n",
            "| Epoch   3 | batch [ 1400/ 2928] | lr 4.51 | ms/batch 37.53 | loss:  5.50 | ppl   245.73\n",
            "| Epoch   3 | batch [ 1600/ 2928] | lr 4.51 | ms/batch 37.55 | loss:  5.53 | ppl   251.25\n",
            "| Epoch   3 | batch [ 1800/ 2928] | lr 4.51 | ms/batch 37.56 | loss:  5.48 | ppl   239.68\n",
            "| Epoch   3 | batch [ 2000/ 2928] | lr 4.51 | ms/batch 37.52 | loss:  5.49 | ppl   241.41\n",
            "| Epoch   3 | batch [ 2200/ 2928] | lr 4.51 | ms/batch 37.55 | loss:  5.36 | ppl   211.91\n",
            "| Epoch   3 | batch [ 2400/ 2928] | lr 4.51 | ms/batch 37.72 | loss:  5.47 | ppl   237.09\n",
            "| Epoch   3 | batch [ 2600/ 2928] | lr 4.51 | ms/batch 37.67 | loss:  5.47 | ppl   238.47\n",
            "| Epoch   3 | batch [ 2800/ 2928] | lr 4.51 | ms/batch 37.59 | loss:  5.41 | ppl   224.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 113.82s | valid loss  5.60 | valid ppl   269.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| Epoch   4 | batch [  200/ 2928] | lr 4.29 | ms/batch 37.72 | loss:  5.44 | ppl   231.35\n",
            "| Epoch   4 | batch [  400/ 2928] | lr 4.29 | ms/batch 37.52 | loss:  5.47 | ppl   238.57\n",
            "| Epoch   4 | batch [  600/ 2928] | lr 4.29 | ms/batch 37.52 | loss:  5.27 | ppl   195.30\n",
            "| Epoch   4 | batch [  800/ 2928] | lr 4.29 | ms/batch 37.58 | loss:  5.34 | ppl   207.77\n",
            "| Epoch   4 | batch [ 1000/ 2928] | lr 4.29 | ms/batch 37.55 | loss:  5.29 | ppl   198.55\n",
            "| Epoch   4 | batch [ 1200/ 2928] | lr 4.29 | ms/batch 37.50 | loss:  5.34 | ppl   208.95\n",
            "| Epoch   4 | batch [ 1400/ 2928] | lr 4.29 | ms/batch 37.60 | loss:  5.35 | ppl   211.15\n",
            "| Epoch   4 | batch [ 1600/ 2928] | lr 4.29 | ms/batch 37.46 | loss:  5.38 | ppl   217.89\n",
            "| Epoch   4 | batch [ 1800/ 2928] | lr 4.29 | ms/batch 37.45 | loss:  5.34 | ppl   208.45\n",
            "| Epoch   4 | batch [ 2000/ 2928] | lr 4.29 | ms/batch 37.44 | loss:  5.35 | ppl   210.19\n",
            "| Epoch   4 | batch [ 2200/ 2928] | lr 4.29 | ms/batch 37.56 | loss:  5.21 | ppl   183.68\n",
            "| Epoch   4 | batch [ 2400/ 2928] | lr 4.29 | ms/batch 37.37 | loss:  5.33 | ppl   206.27\n",
            "| Epoch   4 | batch [ 2600/ 2928] | lr 4.29 | ms/batch 37.47 | loss:  5.34 | ppl   208.87\n",
            "| Epoch   4 | batch [ 2800/ 2928] | lr 4.29 | ms/batch 37.54 | loss:  5.28 | ppl   195.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 113.62s | valid loss  5.51 | valid ppl   246.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| Epoch   5 | batch [  200/ 2928] | lr 4.07 | ms/batch 37.74 | loss:  5.32 | ppl   204.17\n",
            "| Epoch   5 | batch [  400/ 2928] | lr 4.07 | ms/batch 37.54 | loss:  5.35 | ppl   210.71\n",
            "| Epoch   5 | batch [  600/ 2928] | lr 4.07 | ms/batch 37.53 | loss:  5.15 | ppl   172.27\n",
            "| Epoch   5 | batch [  800/ 2928] | lr 4.07 | ms/batch 37.56 | loss:  5.22 | ppl   184.41\n",
            "| Epoch   5 | batch [ 1000/ 2928] | lr 4.07 | ms/batch 37.80 | loss:  5.18 | ppl   177.13\n",
            "| Epoch   5 | batch [ 1200/ 2928] | lr 4.07 | ms/batch 37.46 | loss:  5.22 | ppl   185.72\n",
            "| Epoch   5 | batch [ 1400/ 2928] | lr 4.07 | ms/batch 37.59 | loss:  5.25 | ppl   190.34\n",
            "| Epoch   5 | batch [ 1600/ 2928] | lr 4.07 | ms/batch 37.66 | loss:  5.28 | ppl   196.55\n",
            "| Epoch   5 | batch [ 1800/ 2928] | lr 4.07 | ms/batch 37.52 | loss:  5.24 | ppl   187.77\n",
            "| Epoch   5 | batch [ 2000/ 2928] | lr 4.07 | ms/batch 37.83 | loss:  5.25 | ppl   189.63\n",
            "| Epoch   5 | batch [ 2200/ 2928] | lr 4.07 | ms/batch 37.81 | loss:  5.10 | ppl   164.27\n",
            "| Epoch   5 | batch [ 2400/ 2928] | lr 4.07 | ms/batch 37.83 | loss:  5.22 | ppl   185.48\n",
            "| Epoch   5 | batch [ 2600/ 2928] | lr 4.07 | ms/batch 37.70 | loss:  5.23 | ppl   186.53\n",
            "| Epoch   5 | batch [ 2800/ 2928] | lr 4.07 | ms/batch 37.66 | loss:  5.17 | ppl   176.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 114.03s | valid loss  5.55 | valid ppl   258.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| Epoch   6 | batch [  200/ 2928] | lr 3.87 | ms/batch 37.60 | loss:  5.21 | ppl   183.62\n",
            "| Epoch   6 | batch [  400/ 2928] | lr 3.87 | ms/batch 37.56 | loss:  5.24 | ppl   189.47\n",
            "| Epoch   6 | batch [  600/ 2928] | lr 3.87 | ms/batch 37.62 | loss:  5.05 | ppl   156.39\n",
            "| Epoch   6 | batch [  800/ 2928] | lr 3.87 | ms/batch 37.70 | loss:  5.12 | ppl   168.01\n",
            "| Epoch   6 | batch [ 1000/ 2928] | lr 3.87 | ms/batch 37.72 | loss:  5.08 | ppl   161.51\n",
            "| Epoch   6 | batch [ 1200/ 2928] | lr 3.87 | ms/batch 37.61 | loss:  5.13 | ppl   168.77\n",
            "| Epoch   6 | batch [ 1400/ 2928] | lr 3.87 | ms/batch 37.68 | loss:  5.15 | ppl   172.35\n",
            "| Epoch   6 | batch [ 1600/ 2928] | lr 3.87 | ms/batch 37.54 | loss:  5.18 | ppl   177.34\n",
            "| Epoch   6 | batch [ 1800/ 2928] | lr 3.87 | ms/batch 37.62 | loss:  5.14 | ppl   171.13\n",
            "| Epoch   6 | batch [ 2000/ 2928] | lr 3.87 | ms/batch 37.63 | loss:  5.14 | ppl   171.14\n",
            "| Epoch   6 | batch [ 2200/ 2928] | lr 3.87 | ms/batch 37.63 | loss:  5.01 | ppl   149.86\n",
            "| Epoch   6 | batch [ 2400/ 2928] | lr 3.87 | ms/batch 37.55 | loss:  5.12 | ppl   167.66\n",
            "| Epoch   6 | batch [ 2600/ 2928] | lr 3.87 | ms/batch 37.73 | loss:  5.14 | ppl   170.73\n",
            "| Epoch   6 | batch [ 2800/ 2928] | lr 3.87 | ms/batch 37.55 | loss:  5.11 | ppl   165.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 113.90s | valid loss  5.53 | valid ppl   253.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| Epoch   7 | batch [  200/ 2928] | lr 3.68 | ms/batch 37.83 | loss:  5.12 | ppl   167.14\n",
            "| Epoch   7 | batch [  400/ 2928] | lr 3.68 | ms/batch 37.69 | loss:  5.15 | ppl   172.04\n",
            "| Epoch   7 | batch [  600/ 2928] | lr 3.68 | ms/batch 37.60 | loss:  4.97 | ppl   143.69\n",
            "| Epoch   7 | batch [  800/ 2928] | lr 3.68 | ms/batch 37.63 | loss:  5.02 | ppl   151.85\n",
            "| Epoch   7 | batch [ 1000/ 2928] | lr 3.68 | ms/batch 37.48 | loss:  4.99 | ppl   146.29\n",
            "| Epoch   7 | batch [ 1200/ 2928] | lr 3.68 | ms/batch 37.44 | loss:  5.04 | ppl   154.87\n",
            "| Epoch   7 | batch [ 1400/ 2928] | lr 3.68 | ms/batch 37.38 | loss:  5.06 | ppl   157.69\n",
            "| Epoch   7 | batch [ 1600/ 2928] | lr 3.68 | ms/batch 37.49 | loss:  5.09 | ppl   162.66\n",
            "| Epoch   7 | batch [ 1800/ 2928] | lr 3.68 | ms/batch 37.45 | loss:  5.06 | ppl   157.19\n",
            "| Epoch   7 | batch [ 2000/ 2928] | lr 3.68 | ms/batch 37.45 | loss:  5.06 | ppl   158.22\n",
            "| Epoch   7 | batch [ 2200/ 2928] | lr 3.68 | ms/batch 37.42 | loss:  4.92 | ppl   137.27\n",
            "| Epoch   7 | batch [ 2400/ 2928] | lr 3.68 | ms/batch 37.40 | loss:  5.04 | ppl   153.84\n",
            "| Epoch   7 | batch [ 2600/ 2928] | lr 3.68 | ms/batch 37.44 | loss:  5.05 | ppl   155.67\n",
            "| Epoch   7 | batch [ 2800/ 2928] | lr 3.68 | ms/batch 37.48 | loss:  5.00 | ppl   147.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 113.57s | valid loss  5.49 | valid ppl   241.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| Epoch   8 | batch [  200/ 2928] | lr 3.49 | ms/batch 37.69 | loss:  5.04 | ppl   154.98\n",
            "| Epoch   8 | batch [  400/ 2928] | lr 3.49 | ms/batch 37.40 | loss:  5.05 | ppl   156.29\n",
            "| Epoch   8 | batch [  600/ 2928] | lr 3.49 | ms/batch 37.39 | loss:  4.88 | ppl   131.72\n",
            "| Epoch   8 | batch [  800/ 2928] | lr 3.49 | ms/batch 37.44 | loss:  4.95 | ppl   141.57\n",
            "| Epoch   8 | batch [ 1000/ 2928] | lr 3.49 | ms/batch 37.49 | loss:  4.91 | ppl   134.98\n",
            "| Epoch   8 | batch [ 1200/ 2928] | lr 3.49 | ms/batch 37.49 | loss:  4.97 | ppl   143.36\n",
            "| Epoch   8 | batch [ 1400/ 2928] | lr 3.49 | ms/batch 37.49 | loss:  4.98 | ppl   146.00\n",
            "| Epoch   8 | batch [ 1600/ 2928] | lr 3.49 | ms/batch 37.54 | loss:  5.01 | ppl   150.19\n",
            "| Epoch   8 | batch [ 1800/ 2928] | lr 3.49 | ms/batch 37.56 | loss:  4.98 | ppl   146.16\n",
            "| Epoch   8 | batch [ 2000/ 2928] | lr 3.49 | ms/batch 37.55 | loss:  4.98 | ppl   145.43\n",
            "| Epoch   8 | batch [ 2200/ 2928] | lr 3.49 | ms/batch 37.74 | loss:  4.83 | ppl   125.66\n",
            "| Epoch   8 | batch [ 2400/ 2928] | lr 3.49 | ms/batch 37.54 | loss:  4.96 | ppl   141.92\n",
            "| Epoch   8 | batch [ 2600/ 2928] | lr 3.49 | ms/batch 37.55 | loss:  4.97 | ppl   143.80\n",
            "| Epoch   8 | batch [ 2800/ 2928] | lr 3.49 | ms/batch 37.61 | loss:  4.91 | ppl   136.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 113.65s | valid loss  5.50 | valid ppl   243.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| Epoch   9 | batch [  200/ 2928] | lr 3.32 | ms/batch 37.79 | loss:  4.96 | ppl   143.19\n",
            "| Epoch   9 | batch [  400/ 2928] | lr 3.32 | ms/batch 37.57 | loss:  4.98 | ppl   145.64\n",
            "| Epoch   9 | batch [  600/ 2928] | lr 3.32 | ms/batch 37.62 | loss:  4.81 | ppl   122.13\n",
            "| Epoch   9 | batch [  800/ 2928] | lr 3.32 | ms/batch 37.53 | loss:  4.88 | ppl   131.31\n",
            "| Epoch   9 | batch [ 1000/ 2928] | lr 3.32 | ms/batch 37.53 | loss:  4.84 | ppl   126.38\n",
            "| Epoch   9 | batch [ 1200/ 2928] | lr 3.32 | ms/batch 37.54 | loss:  4.90 | ppl   134.15\n",
            "| Epoch   9 | batch [ 1400/ 2928] | lr 3.32 | ms/batch 37.56 | loss:  4.91 | ppl   135.33\n",
            "| Epoch   9 | batch [ 1600/ 2928] | lr 3.32 | ms/batch 37.52 | loss:  4.94 | ppl   139.77\n",
            "| Epoch   9 | batch [ 1800/ 2928] | lr 3.32 | ms/batch 37.61 | loss:  4.91 | ppl   135.89\n",
            "| Epoch   9 | batch [ 2000/ 2928] | lr 3.32 | ms/batch 37.57 | loss:  4.91 | ppl   135.33\n",
            "| Epoch   9 | batch [ 2200/ 2928] | lr 3.32 | ms/batch 37.62 | loss:  4.76 | ppl   117.22\n",
            "| Epoch   9 | batch [ 2400/ 2928] | lr 3.32 | ms/batch 37.56 | loss:  4.88 | ppl   132.18\n",
            "| Epoch   9 | batch [ 2600/ 2928] | lr 3.32 | ms/batch 37.57 | loss:  4.90 | ppl   133.91\n",
            "| Epoch   9 | batch [ 2800/ 2928] | lr 3.32 | ms/batch 37.58 | loss:  4.84 | ppl   127.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 113.80s | valid loss  5.50 | valid ppl   244.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| Epoch  10 | batch [  200/ 2928] | lr 3.15 | ms/batch 37.89 | loss:  4.89 | ppl   132.94\n",
            "| Epoch  10 | batch [  400/ 2928] | lr 3.15 | ms/batch 37.77 | loss:  4.91 | ppl   135.34\n",
            "| Epoch  10 | batch [  600/ 2928] | lr 3.15 | ms/batch 37.53 | loss:  4.74 | ppl   114.25\n",
            "| Epoch  10 | batch [  800/ 2928] | lr 3.15 | ms/batch 37.54 | loss:  4.80 | ppl   121.92\n",
            "| Epoch  10 | batch [ 1000/ 2928] | lr 3.15 | ms/batch 37.54 | loss:  4.78 | ppl   118.77\n",
            "| Epoch  10 | batch [ 1200/ 2928] | lr 3.15 | ms/batch 37.55 | loss:  4.83 | ppl   125.41\n",
            "| Epoch  10 | batch [ 1400/ 2928] | lr 3.15 | ms/batch 37.48 | loss:  4.84 | ppl   126.50\n",
            "| Epoch  10 | batch [ 1600/ 2928] | lr 3.15 | ms/batch 37.52 | loss:  4.88 | ppl   131.56\n",
            "| Epoch  10 | batch [ 1800/ 2928] | lr 3.15 | ms/batch 37.47 | loss:  4.85 | ppl   127.79\n",
            "| Epoch  10 | batch [ 2000/ 2928] | lr 3.15 | ms/batch 37.55 | loss:  4.84 | ppl   126.96\n",
            "| Epoch  10 | batch [ 2200/ 2928] | lr 3.15 | ms/batch 37.57 | loss:  4.70 | ppl   110.45\n",
            "| Epoch  10 | batch [ 2400/ 2928] | lr 3.15 | ms/batch 37.55 | loss:  4.82 | ppl   123.56\n",
            "| Epoch  10 | batch [ 2600/ 2928] | lr 3.15 | ms/batch 37.51 | loss:  4.83 | ppl   125.08\n",
            "| Epoch  10 | batch [ 2800/ 2928] | lr 3.15 | ms/batch 37.51 | loss:  4.78 | ppl   118.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 113.78s | valid loss  5.52 | valid ppl   250.21\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
        "      f'test ppl {test_ppl:8.2f}')\n",
        "print('=' * 89)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeufzeCtlyfB",
        "outputId": "30accdaf-6fc7-491b-adee-74746a3decd4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  5.40 | test ppl   221.78\n",
            "=========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Pl8fvt0Uqojl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}